# 从策略梯度法到Actor-Critic

上文搞懂了策略梯度法，是现代强化学习方法的基石。后续的算法，都是在策略梯度法上进行改进。

## 基础的策略梯度法有什么问题？

回顾一下基础的策略梯度法：

1. 构建了策略神经网络参数与累积奖励期望的关系。
2. 通过构造代理损失函数：
   ```math
   \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) G_t \right]
   ```
   来调整向让更大的方向移动。

这其中有一个关键步骤：**将期望的梯度替换为了梯度的期望**。

之所以要这么做，是因为环境模型往往未知（状态转移函数、奖励函数），所以需要转换成梯度的期望，从而可以用蒙特卡洛法对采样取平均值来逼近。  
但蒙特卡洛法有一个缺点，就是用真实数据采样包含了很大的随机性（方差高）。而我们强化学习要做的，就是想通过建模发现/挖掘数据的规律，从而可以预测。方差高就意味着更难发现数据中的规律。这就产生了矛盾。

总结一下因果：  
强化学习用奖励的期望做目标建模  
-> 但期望依赖的环境模型（状态转移概率等）不可知  
-> 通过变换消掉状态转移概率+使用蒙特卡洛法采样真实数据近似解决  
-> 但采样数据随机性高方差大难估计  

因此，后续的优化方向就是：**在保证奖励期望不变的情况下，降低数据的方差，提升估计的准确度**。

---

## Reward-To-Go法降方差

reward to go从$`G_t`$的构成入手优化，尝试消掉其中没用的部分（噪声）。  
看公式：
```math
G_t = \sum_{k=t}^T \gamma^{k-t} r_k
```
包含了整个回合的折扣累计收益。  
从感性上认知，我在时刻$`t`$做了动作，获取到奖励，影响的范围是从时刻$`t`$起到回合结束的奖励，和从0～$`t`$的累计奖励无关。但损失函数里用的却是$`G_t`$，通过去掉时刻$`t`$之前的累积奖励，可以降低噪声。  
这是感性上的认知，还得从数学上证明：$`t`$时刻历史累积奖励的期望为0，但方差不为0。

观察梯度公式：
```math
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) G_t \right]
```

这里需要证明历史奖励的期望贡献是0，但方差不为0。  
将$`G_t`$代入：
```math
G_t = \sum_{k=0}^{t-1} \gamma^k r_k + \sum_{k=t}^T \gamma^{k-t} r_k
```

将累积奖励分为t时刻以前的历史奖励和t时刻及之后的未来奖励展开代入：
```math
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \left( \sum_{k=0}^{t-1} \gamma^k r_k + \sum_{k=t}^T \gamma^{k-t} r_k \right) \right]
```

可以证明，前一项（历史奖励）期望为0，但方差不为零，可以去掉。  
具体的数学证明过程暂时跳过。  
经过调整后：
```math
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \sum_{k=t}^T \gamma^{k-t} r_k \right]
```

---

## Baseline法降方差

baseline法降低方差的思路挺有意思，通过一个示例说明。

如图，以$`y = \bar{G}`$为基准，生成了一些随机点，平均值为9.63，方差为151.08。  
方差计算公式：
```math
Var = \frac{1}{N} \sum_{i=1}^N (G_i - \bar{G})^2
```
其中，$` \bar{G} = \frac{1}{N} \sum_{i=1}^N G_i `$。  
从公式可以看到，计算方差的基准是平均值。但是一条直线，并不能体现出随机点的趋势。  
再看另一个曲线：

通过对随机点拟合，拟合出的函数为$` V_\phi(s) `$（图中蓝色的线），这样，每个点都有了一个拟合的$` V_\phi(s) `$，会体现出随机点的趋势。以$` V_\phi(s) `$为基准计算方差：
```math
Var = \frac{1}{N} \sum_{i=1}^N (G_i - V_\phi(s_i))^2
```
得到的方差为20.41，远小于以平均值为基准的151.08。  
感性理解，我们想通过方差了解到的，是随机点在“趋势”附近的波动程度，用平均值为基准的方差，把“趋势”本身的波动也给计算在内了。所以，可以用包含了趋势信息的预估值代替平均值来做基准，降低方差。

这个原理应用到强化学习上，来降低累积收益的方差：
```math
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) (G_t - V_\phi(s_t)) \right]
```
其中$` V_\phi(s_t) `$是拟合值，这样可以降低累积收益的方差。  
但这样会影响期望的值吗？答案是不会，可以证明，$` V_\phi(s_t) `$可以是独立于动作的任何值（不能是$` a_t `$的函数），都不会影响。  
展开公式：
```math
\mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a_t|s_t) V_\phi(s_t) \right] = 0
```

数学证明过程暂时跳过，后续补充。

---

## 构建Actor-Critic模型

从上文中我们知道，用真实值减去拟合值，可以降低方差，且对期望没有影响。  
看$` G_t `$的定义：
```math
G_t = \sum_{k=t}^T \gamma^{k-t} r_k
```
经过Reward To Go优化，已调整为当前状态到回合结束的累积收益。

现在来针对$` V_\phi(s) `$建模。创建一个神经网络，输入是当前状态，输出是当前状态的价值，$` V_\phi(s) `$的label即真实累积奖励$` G_t `$。loss为$` G_t `$和预测值的均方差：
```math
\mathcal{L}(\phi) = \frac{1}{2} \| G_t - V_\phi(s_t) \|^2
```
通过对loss反向传播，让预测值不断逼近真实的累积奖励。  
这就是**Critic**的建模。

再关注回策略更新，将$` G_t - V_\phi(s_t) `$代入策略梯度法的损失函数：
```math
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) (G_t - V_\phi(s_t)) \right]
```
这就是**Actor-Critic算法**，Critic网络预测状态价值，作为baseline，在不改变期望的情况下，降低真实累积奖励的方差。

---

## TD法优化模型

以上都在使用蒙特卡洛法，简单说，使用蒙特卡洛法的场景，基本都能用TD法代替，且TD法的收敛速度更快。  
这块不再对TD法进行详细推导，TD法的理解涉及动态规划法和蒙特卡洛法的原理，解释篇幅比较长，可以复习书的6.1节，如果比较生疏，可以再倒回4.1节和5.1节，重新理解下动态规划和蒙特卡洛的概念。这里只简单介绍下。  
动态规划法是通过预测的下一状态的累积奖励值期望，来引导当前状态的价值。而蒙特卡洛法是用真实采样出来的未来所有状态累积奖励，来引导当前状态的价值。两者二合一，就成了TD法：通过预测的下一状态的累积奖励值，来引导当前状态的价值。  
由于TD法只用了预测的下一状态的值，方差天然比蒙特卡洛法用以后所有状态累积奖励小。  
用蒙特卡洛法在工程实现上还有一个问题是，$` G_t `$的未来累积奖励需要等游戏回合结束后才能获取到，期间的奖励数据需要存下来，给存储和计算都造成了压力。  

因此，改进critic模型用TD法。TD法的label不再是采样的真实的未来累积奖励，而是真实累积奖励的有偏估计值：
```math
y_t = r_t + \gamma V_\phi(s_{t+1})
```
其中$` V_\phi(s_{t+1}) `$是用神经网络预测的，下一状态的未来累积奖励。  
用TD法的损失函数如下：
```math
\mathcal{L}(\phi) = \frac{1}{2} \| y_t - V_\phi(s_t) \|^2
```

---

## 总结

至此，从基础的策略梯度法，迭代到了Actor-Critic算法。  
其核心思想，就是因为环境模型未知，所以用蒙特卡洛法采样近似，从而到导致方差高，各种优化都围绕在如何期望不变的情况下降低方差的角度展开。