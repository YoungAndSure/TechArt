#### 公式3替代了公式2，在实际任务中是怎么做的？  
迭代的流程是这样的，先根据策略$`\pi`$行动，行动过程中对于$`s,a`$访问的概率权重是$`\rho_{\pi}`$,收集样本，计算平均期望，求策略梯度，更新策略到$`\tilde{\pi}`$。  
说的是，在更新到$`\tilde{\pi}`$之前，想评估一下策略提升前后的平均奖励提升幅度。公式就是：  
```math
\eta(\tilde{\pi})=\eta(\pi) + \sum_{s}{\rho}_{\tilde{\pi}}(s)\sum_{a}\tilde{\pi}(s,a)A_{\pi}(s,a)
```
其中访问哪个状态以及选择哪个动作，都跟新策略有关，所以是$`\tilde{\rho}_{\pi}`$和$`\tilde{\pi}(s,a)`$,而行动价值$`q_{\pi}(s,a)`$和状态价值$`v_{\pi}(s,a)`$都是在策略行动$`\pi`$时计算出来的。也就是说在老策略的价值体系下用新策略评估平均奖励提升了多少。（注意，$`q_{\pi}(s,a)`$可是依赖了未来的状态价值，也是经过策略$`\pi`$行动、迭代收敛来的，所以可不能写成$`A_{\tilde{\pi}}(s,a)`$。  
再理解下，为什么是$`A_{\pi}(s,a)`$。如果固定策略$`\pi`$，一直按照策略$`\pi`$行动，收集样本，计算行动价值和状态价值，最终会收敛到策略$`\pi`$下的行动价值和状态价值，也就是贝尔曼方程固定策略时状态价值的解。按照传统动态规划中迭代法的方法，这时候可以按照贪婪策略调整策略$`\pi`$，得到最优策略，然后继续按照新策略行动，再次收敛。。循环往复，最终解出贝尔曼最优方程。而策略梯度法是更新策略，但不是贪婪策略，因为它还要考虑探索性。其实和动态规划是一个原理。而新的策略，在前策略得到的状态价值上，应该得到更优的状态价值。在上面公式中，也就是说加号后面的部分要大于0。  
  
说跑题了。为了评估新策略对旧策略的提升，可以按照上式计算加号后部分。如果智能体真的按照新策略行动一遍，但不更新行动价值和状态价值，就能计算出来，且状态权重概率是$`\rho_{\tilde{\pi}}`$。但现实中比较困难，所以无法得到新策略下的$`\rho_{\tilde{\pi}}`$，而是用旧策略行动中得到$`\rho_{\pi}`$代替。  

#### 实际应用中是怎么计算替代目标的
替代目标就是：  
```math
L_{\pi}(\tilde{\pi}) = \eta_{\pi} + \sum_{s}\rho_{\pi}(s)\sum_a \tilde{\pi}(s,a)A_{\pi}(s,a)
```
因为新的策略必须要让智能体走一遍才能采集到真实的样本，计算出新策略下的累计折扣奖励期望$`\eta_{\tilde{\pi}}`$，难实现。所以可以用旧策略下采集到样本，结合新策略，来计算出一个近似的替代目标$`L_{\pi}(\tilde{\pi})`$。由于用的是旧策略的样本，所以它的状态访问概率密度是旧策略的$`\rho_{\pi}`$，而不是$`\rho_{\tilde{\pi}}`$。  
所以，替代目标计算方法就等于用旧策略下样本，计算新策略下目标，通过重要性采样就可以实现。也就是：  
```math
L_{\pi}(\tilde{\pi}) = \eta_{\pi} + \frac{1}{N}\sum_{i=1}^N \frac{\pi(s_i,a_i)}{\tilde{\pi}(s_i,a_i)} A_{\pi}(s_i,a_i)
```

#### 为什么要限制策略更新幅度？
相对优势有正有负，这都正常，策略更新做的就是提升正优势的概率，降低负优势的概率。  
真正的问题在于：  
- 样本方差大导致的估计偏差。如果样本方差大，会导致估计的期望大幅偏离真实期望，产生估计偏差。  
- method approximation 导致的估计误差。函数逼近法本身的缺陷。  

这些都可能导致优势的估计错误。这时候如果完全按照估计值更新策略，就会放大错误，后续需要很多次正确迭代逐渐修复，如果误差持续累积，就会导致离真实期望越来越远。  

#### Kakade & Langford 证明的下界怎么来的  
几个符号概念：  
$`\pi_{old}`$:老策略  
$`\pi_{new}`$:新策略  
$`L_{\pi_{old}}(\pi_{old})`$:在老策略$`\pi_{old}`$下得到的估计目标，其实就等于真实的目标$`\eta(\pi_{old})`$  
$`L_{\pi_{old}}(\pi_{new})`$:在老策略$`\pi_{old}`$得到的价值体系下，应用新策略$`\pi_{new}`$得到的估计目标，理论上小于在新策略上运行得到的真实目标$`\eta(\pi_{new})`$  
$`\pi'`$:$`\pi'=\argmax L_{\pi_{old}}(\pi_{old})`$，根据老策略计算出的目标，应用贪婪策略得到的策略，更新幅度会很大，注意，它等于$`\pi'=\argmax \eta(\pi_{old})`$,如果按照贪婪策略更新策略，$`\pi_{new}=\pi'`$了，现在是要取其中。由于是贪婪策略，所以必然$`L_{\pi_{old}}(\pi') \ge L_{\pi_{old}}(\pi_{old})`$  
论文是说，如果新策略是老策略和最优策略的折中：  
```math
\pi_{new}(s,a) = (1-\alpha)\pi_{old}(s,a) + \alpha\pi'(s,a)
```
那么：  
```math
\begin{align}
\eta(\pi_{\text{new}}) &\geq L_{\pi_{\text{old}}}(\pi_{\text{new}}) - \frac{2\epsilon\gamma}{(1 - \gamma)^2} \alpha^2\\
\eta(\pi_{\text{new}}) - \eta(\pi_{\text{old}}) &\geq L_{\pi_{\text{old}}}(\pi_{\text{new}}) - \eta(\pi_{\text{old}}) - \frac{2\epsilon\gamma}{(1 - \gamma)^2} \alpha^2\\
\eta(\pi_{\text{new}}) - \eta(\pi_{\text{old}}) &\geq L_{\pi_{\text{old}}}(\pi_{\text{new}}) - L_{\pi_{old}}(\pi_{old}) - \frac{2\epsilon\gamma}{(1 - \gamma)^2} \alpha^2\\
\end{align}
```
因此，结论就是，如果按照以上折中的方式更新策略，目标更新的下界就是：  
```math
L_{\pi_{\text{old}}}(\pi_{\text{new}}) - L_{\pi_{old}}(\pi_{old}) - \frac{2\epsilon\gamma}{(1 - \gamma)^2} \alpha^2
```
