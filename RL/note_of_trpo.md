#### 公式3替代了公式2，在实际任务中是怎么做的？  
迭代的流程是这样的，先根据策略$`\pi`$行动，行动过程中对于$`s,a`$访问的概率权重是$`\rho_{\pi}`$,收集样本，计算平均期望，求策略梯度，更新策略到$`\tilde{\pi}`$。  
说的是，在更新到$`\tilde{\pi}`$之前，想评估一下策略提升前后的平均奖励提升幅度。公式就是：  
```math
\eta(\tilde{\pi})=\eta(\pi) + \sum_{s}\tilde{\rho}_{\pi}(s)\sum_{a}\tilde{\pi}(s,a)A_{\pi}(s,a)
```
其中访问哪个状态以及选择哪个动作，都跟新策略有关，所以是$`\tilde{\rho}_{\pi}`$和$`\tilde{\pi}(s,a)`$,而行动价值$`q_{\pi}(s,a)`$和状态价值$`v_{\pi}(s,a)`$都是在策略行动$`\pi`$时计算出来的。也就是说在老策略的价值体系下用新策略评估平均奖励提升了多少。（注意，$`q_{\pi}(s,a)`$可是依赖了未来的状态价值，也是经过策略$`\pi`$行动、迭代收敛来的，所以可不能写成$`A_{\tilde{\pi}}(s,a)`$。  
再理解下，为什么是$`A_{\pi}(s,a)`$。如果固定策略$`\pi`$，一直按照策略$`\pi`$行动，收集样本，计算行动价值和状态价值，最终会收敛到策略$`\pi`$下的行动价值和状态价值，也就是贝尔曼方程固定策略时状态价值的解。按照传统动态规划中迭代法的方法，这时候可以按照贪婪策略调整策略$`\pi`$，得到最优策略，然后继续按照新策略行动，再次收敛。。循环往复，最终解出贝尔曼最优方程。而策略梯度法是更新策略，但不是贪婪策略，因为它还要考虑探索性。其实和动态规划是一个原理。而新的策略，在前策略得到的状态价值上，应该得到更优的状态价值。在上面公式中，也就是说加号后面的部分要大于0。  
