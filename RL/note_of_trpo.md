#### 公式3替代了公式2，在实际任务中是怎么做的？  
迭代的流程是这样的，先根据策略$`\pi`$行动，行动过程中对于$`s,a`$访问的概率权重是$`\rho_{\pi}`$,收集样本，计算平均期望，求策略梯度，更新策略到$`\tilde{\pi}`$。  
说的是，在更新到$`\tilde{\pi}`$之前，想评估一下策略提升前后的平均奖励提升幅度。公式就是：  
```math
\eta(\tilde{\pi})=\eta(\pi) + \sum_{s}{\rho}_{\tilde{\pi}}(s)\sum_{a}\tilde{\pi}(s,a)A_{\pi}(s,a)
```
其中访问哪个状态以及选择哪个动作，都跟新策略有关，所以是$`\tilde{\rho}_{\pi}`$和$`\tilde{\pi}(s,a)`$,而行动价值$`q_{\pi}(s,a)`$和状态价值$`v_{\pi}(s,a)`$都是在策略行动$`\pi`$时计算出来的。也就是说在老策略的价值体系下用新策略评估平均奖励提升了多少。（注意，$`q_{\pi}(s,a)`$可是依赖了未来的状态价值，也是经过策略$`\pi`$行动、迭代收敛来的，所以可不能写成$`A_{\tilde{\pi}}(s,a)`$。  
再理解下，为什么是$`A_{\pi}(s,a)`$。如果固定策略$`\pi`$，一直按照策略$`\pi`$行动，收集样本，计算行动价值和状态价值，最终会收敛到策略$`\pi`$下的行动价值和状态价值，也就是贝尔曼方程固定策略时状态价值的解。按照传统动态规划中迭代法的方法，这时候可以按照贪婪策略调整策略$`\pi`$，得到最优策略，然后继续按照新策略行动，再次收敛。。循环往复，最终解出贝尔曼最优方程。而策略梯度法是更新策略，但不是贪婪策略，因为它还要考虑探索性。其实和动态规划是一个原理。而新的策略，在前策略得到的状态价值上，应该得到更优的状态价值。在上面公式中，也就是说加号后面的部分要大于0。  
  
说跑题了。为了评估新策略对旧策略的提升，可以按照上式计算加号后部分。如果智能体真的按照新策略行动一遍，但不更新行动价值和状态价值，就能计算出来，且状态权重概率是$`\rho_{\tilde{\pi}}`$。但现实中比较困难，所以无法得到新策略下的$`\rho_{\tilde{\pi}}`$，而是用旧策略行动中得到$`\rho_{\pi}`$代替。  

#### 实际应用中是怎么计算替代目标的
替代目标就是：  
```math
L_{\pi}(\tilde{\pi}) = \eta_{\pi} + \sum_{s}\rho_{\pi}(s)\sum_a \tilde{\pi}(s,a)A_{\pi}(s,a)
```
因为新的策略必须要让智能体走一遍才能采集到真实的样本，计算出新策略下的累计折扣奖励期望$`\eta_{\tilde{\pi}}`$，难实现。所以可以用旧策略下采集到样本，结合新策略，来计算出一个近似的替代目标$`L_{\pi}(\tilde{\pi})`$。由于用的是旧策略的样本，所以它的状态访问概率密度是旧策略的$`\rho_{\pi}`$，而不是$`\rho_{\tilde{\pi}}`$。  
所以，替代目标计算方法就等于用旧策略下样本，计算新策略下目标，通过重要性采样就可以实现。也就是：  
```math
L_{\pi}(\tilde{\pi}) = \eta_{\pi} + \frac{1}{N}\sum_{i=1}^N \frac{\pi(s_i,a_i)}{\tilde{\pi}(s_i,a_i)} A_{\pi}(s_i,a_i)
```

