# RL

## Part1:建模
强化学习要解决的，是找到行动的最优策略，使收益最大。解决问题的前提是，对问题进行建模，把一个现实问题转化成可以用公式表达的分析框架。在分析框架的基础上，找最优策略。  
马尔科夫决策过程、贝尔曼方程、贝尔曼最优方程，是强化学习对现实问题的建模方法。最终通过对贝尔曼最优方程分析，来获得最好的策略。  

### 马尔科夫决策过程

<img src="./images/test.png" width="30%" alt="">  

如图，九宫格，很经典，讲强化学习都从这个示例讲起。  
要解决的问题是：智能体从左上角start开始行动，如何找一条到右下角的target的最优路径。  
马尔科夫决策过程这么给这个问题建模的：  
#### 状态
首先是设置状态，智能体完成一系列行动，在状态间跳转。  
在这个问题中，共有九个状态：  
<img src="./images/test1.png" width="30%" alt="">  
#### 动作
在每一个状态里，都有5个行动可以选择。
<img src="./images/test3.png" width="30%" alt="">  
$`a_5`$表示留在原地。

#### 策略
把状态和行动结合在一起，在每个状态下，以多大概率选择哪个行动，就是策略。每个状态可以选择不同的策略。比如在$`t`$时刻，状态$`s_1`$的策略可以是：  
```math
\pi(A_t|S_t=s_1) = \begin{cases}
0, & A=a_1 \\
0.5, & A=a_2 \\
0.5, & A=a_3 \\
0, & A=a_4\\
0, & A=a_5
\end{cases}
```
也就是50%的概率往右走，50%的概率往下走，不会往上往左走或呆在原地。
策略是强化学习要优化的目标，也就是找到一个每个状态上最佳的行动概率分布。

#### 状态转移概率
在状态$`S_t`$下，选择了动作$`A_t`$，会跳转到哪个状态，这个分布是状态转移概率。在九宫格这个问题中，状态转移是确定的，因为一旦确定了动作，也就知道跳转到哪了。所以很容易写出来状态转移概率，比如$`t`$时刻，状态$`S_t=s_1`$选择行动$`A_t=a_2`$的状态转移概率：
```math
p(S_{t+1}|S_t=s_1,A=a_2)= \begin{cases}
1.0, & S_{t+1}=s_2 \\
0, & S_{t+1} \neq s_2
\end{cases}
```

#### 奖励
$`t`$时刻，在状态$`S_t`$下，选择了动作$`A_t`$,跳转到下一状态$`S_{t+1}`$，会获得即时奖励$`R_t`$。  
奖励是强化学习中留给人工调整的接口，要鼓励什么、惩罚什么，都是通过奖励的设置，反馈到模型中，模型根据奖励来找最优策略。  
比如九宫格中，我们不希望智能体走进禁区，所以设置禁区的奖励为-1，不希望走到九宫格外面，所以设置走到九宫格外的奖励为-1，希望智能体能走到目标，所以设置目标的奖励为1.  
奖励既可以看作一个条件概率：
```math
p(R_t=r|S_t=s,A_t=a)
```
比如按照以上奖励设置，$`S_t=s_1`$状态下选择动作$`a_2`$跳转到$`S_{t+1}=s_2`$获得奖励的概率分布为：  
```math
p(R_t|S_t=s_1,A_t=a_2) = 
\begin{cases}
1.0, & R_t = 0.0 \\
0.0, & R_t \neq 0.0
\end{cases}
```
也就是，奖励是0.0。在当前这个九宫格问题中，奖励的获取是确定的，也就是只要在状态$`s_1`$执行动作$`a_2`$就一定获取到奖励$`0.0`$。其他问题未必，所以用条件概率表示更通用。  

### 马尔可夫决策过程
至此，完成了这个九宫格问题的马尔可夫决策过程建模。  
参数设置好后，让智能体从start开始按照参数行动，就得到一个马尔可夫序列。比如，按照策略行动之后，智能体可能走过这么一条路径：  
<img src="./images/test4.png" width="30%" alt="">  

也就产生一条马尔可夫链：
```math
S_0=s_1,A_0=a_2,R_0=0.0,\\
S_1=s_2,A_1=a_3,R_1=0.0,\\
S_2=s_5,A_2=a_3,R_2=0.0,\\
S_3=s_8,A_3=a_2,R_3=1.0,\\
S_4=s_9
```

通过马尔可夫决策过程，我们对一个智能体的行动过程进行了建模，可以通过一系列随机变量及概率分布，描述出一个智能体的行动过程。

### 贝尔曼方程
#### 状态价值
找最优策略的目的是让收益最大化，那在智能体行动的过程中，收益该如何定义？  
比如我从状态$`s_1`$出发，在策略、状态转移概率的作用下，可能到达不同的状态，在奖励概率分布的作用下，可能在过程中获取到不同的奖励。  
<img src="./images/test5.png" width="60%" alt="">  
如图，条条大路通$`s_9`$，假设前者路线产生的概率为$`p_1`$, 累积收益为2，后者路线产生的概率为$`p_2`$，累积收益为1.该如何衡量各种情况下，从状态$`s_1`$到$`s_9`$取得的收益？   
  
##### 用期望衡量各种情况下的回报  
如状态$`s_1`$的状态价值可以表示为：  
```math
v(s_1) = p_1 * 2 + p_2 * 1
```
如果用随机变量$`G_t`$来表示$`t`$时刻开始，从状态$`S_t=s`$到回合结束，每条马尔可夫链获得的奖励，则状态价值的定义就是：
```math
v(s)=\mathbb{E}[G_t|S_t=s]
```
也就是累积收益的期望。  
  
##### 累积奖励如何计算？  
上例是用过程中的奖励直接累加，来计算奖励。  
这样有一个问题。  
比如上图二中在$`s_8`$时奖励为+1，由于奖励是累加的，那可以在$`s_8`$时不断选择$`a_5`$，来获取+1的奖励。这个策略显然比在$`s_8`$选择动作$`a_2`$跳转到$`s_9`$获得的奖励要高。但这显然不是最优策略，我们希望最优策略应该是路径最短的。  
因此，奖励计算被设计为2部分，一部分是即时奖励，一部分是未来折扣奖励。对于未来的奖励，需要加一个折扣系数$`\gamma`$:  
```math
G_t = R_t + \gamma G_{t+1} = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2}...
```
这样，步数越往后获得的奖励，对当前状态价值的影响越小。  

至此，可以得到状态价值的定义：
```math
v(S_t=s)=\mathbb{E}[R_t+\gamma G_{t+1}]
```

#### 状态价值展开
根据状态价值的定义，及马尔可夫决策过程中定义的概率分布，可以将状态价值中的即时奖励和未来折扣奖励展开。
```math
v(S_t=s)=\mathbb{E}[R_t+\gamma G_{t+1}|S_t=s]=\mathbb{E}[R_t|S_t=s]+\gamma \mathbb{E}[G_{t+1}|S_t=s]
```
```math
\mathbb{E}[R_t|S_t=s] = \sum_{a\in \mathcal{A}}\pi(A_t=a|S_t=s)\sum_{s'\in\mathcal{S}}p(S_{t+1}=s'|S_t=s,A_t=a)\sum_{r\in\mathcal{R}}p(R_t=r|S_t=s,A_t=a)r
```
即时奖励的展开比较好理解，就是把所有行为、状态转移、奖励分布的概率相乘并加和，获取到即使奖励发生的概率，再乘以奖励就可以了。
```math
\mathbb{E}[G_{t+1}|S_t=s]=\sum_{a\in \mathcal{A}}\pi(A_t=a|S_t=s)\sum_{s'\in\mathcal{S}}p(S_{t+1}=s'|S_t=s,A_t=a)v(s')
```
未来折扣奖励的获取比较绕，需要一段公式变换（这里省略）。变换后的公式不难理解。
```math
\sum_{a\in \mathcal{A}}\pi(A_t=a|S_t=s)\sum_{s'\in\mathcal{S}}p(S_{t+1}=s'|S_t=s,A_t=a)
```
这部分是从状态$`S_t=s`$跳转到$`S_{t+1}=s'`$的概率，后面的$`v(s')`$是下一个状态的状态价值。$`v(s')`$也是一个期望，来衡量所有情况下从下一个状态$`s'`$到回合结束的奖励。

即时奖励和未来折扣奖励合并后：
```math
v(s)=\sum_{a\in \mathcal{A}}\sum_{s'\in\mathcal{S}}\pi(A_t=a|S_t=s)p(S_{t+1}=s'|A_t=a,S_t=s)[\sum_{r\in\mathcal{R}}p(R_t=r|S_t=s,A_t=a)r+\gamma v(s')]
```
由于在大多数问题里，即时奖励的获取由状态$`S_t`$和行动$`A_t`$、下一状态$`S_{t+1}`$确定，可以直接看作一个函数$`r(s,a,s')`$，因此可以简化为：  
```math
v(s)=\sum_{a\in \mathcal{A}}\sum_{s'\in\mathcal{S}}\pi(A_t=a|S_t=s)p(S_{t+1}=s'|A_t=a,S_t=s)[r(s,a,s')+\gamma v(s')]
```
公式分成两部分，一部分是前面的概率，由策略$`\pi`$和状态转移概率$`p`$组成。  
一部分是后面的奖励，由即时奖励$`r`$和未来折扣奖励$`\gamma v(s')`$组成。  

#### 贝尔曼方程
仔细看状态价值的公式：  
```math
v(s)=\sum_{a\in \mathcal{A}}\sum_{s'\in\mathcal{S}}\pi(A_t=a|S_t=s)p(S_{t+1}=s'|A_t=a,S_t=s)[r(s,a,s')+\gamma v(s')]
```
可以发现，$`v(s)`$和$`v(s')`$都是未知待求的变量，其他概率分布都是已知的参数。也就是说，它是一个方程的形式，类似：
```math
x = a(b+x)
```
因此，它是一个方程。这就是贝尔曼方程。

#### 解贝尔曼方程
在已知策略和状态转移概率的情况下，贝尔曼方程可以通过矩阵运算解出。  
因为：
- $`v(s)`$是个$`|\mathcal{S}| \times 1`$的向量
- 策略$`\pi`$是个$`|\mathcal{S}| \times |\mathcal{A}|`$的矩阵
- 状态转移概率$`p`$是个$`|\mathcal{S}| \times |\mathcal{S}|`$的矩阵
- 奖励$`r`$实际上只和转移后的状态有关，是个$`|\mathcal{S}| \times 1`$的向量
- $`v(s')`$和$`v(s)`$一样，是个$`|\mathcal{S}| \times 1`$的向量

其中$`v(s)`$和$`v(s')`$都是未知待求的变量，其他都是已知的参数，可以看到，


