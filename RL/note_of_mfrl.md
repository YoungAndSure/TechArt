# 强化学习的数学原理 笔记

#### 先说一个idea
没有一个好用的、给pdf做笔记的软件。
有很多pdf编辑软件，但编辑的都是本地文件，需要一直保存这个文件才行。或者说，构建的是笔记-pdf文件的关系。一旦文件丢失，笔记也丢失。  
读书软件，如微信读书，pdf不能编辑、做笔记。pdf可以自动转成电子书的格式，可以做笔记了，但是转换过来之后排版还是不好，更不用说有的公式会丢失、错误了。这个排版问题可能很难完美解决，再怎么转都不如pdf原生的排版好。这种的笔记属于托管到平台了，平台保证笔记和书的对应关系。  
希望有这么一个软件：  
1. 可以对pdf做笔记，划线写想法那种，类似微信读书。  
2. 构建的是内容块和笔记的关系。也就是说，这个笔记锚住的是内容块——即使这个pdf丢了，我又从网上下载了一个，只要上传，依然可以把保存的笔记和内容对应上。  

谁来做个这样的app，我就不用在另外一个地方记笔记了。  

#### 2.2节引出自举、贝尔曼方程
这节挺有意思。通过一个简单示例，引出了不同状态回报之间的关系，通过自举，很清楚的构建起了各状态回报的方程，展示了贝尔曼方程的要义。

#### 2.3节开头提到，回报不适合评估随机系统，为什么？
随机系统在某个状态下选择的动作是随机的，导致相同状态下获取到的回报不同。  
回报是整个回合的累计奖励，是从回合的视角来评估的，通过比较回报，可以知道这个回合的轨迹更好、那个回合的轨迹差一些。  
但在实际运行过程中，是基于某个状态来选动作的，如果用回报来指导某个状态该如何选择动作，会发现一个状态对应了很多个回报，且由于随机性，这些回报的方差很大，这就很难决策在当前状态下到底哪个动作是最好的了。  
根本原因是，随机系统中，回报只能显示整个轨迹的好坏，且带有很大随机性，导致难找到规律。又无法把这个规律反馈到轨迹中的某一步中去优化决策。

#### 回报和状态价值有什么区别？
回报是整条轨迹的折扣奖励累积：  
``` math
return = R_0 + \gamma R_1 + \gamma^2 R_2 ....
```
而状态价值是某状态之后（t时刻起）折扣奖励累积——的期望：  
```math
v_\pi(s) = E(G_t|S_t=s)
```
```math
  G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}....
```
有两点不同：
- 状态价值是状态的函数，只和状态之后的奖励相关
- 状态价值是期望

书中也说了，如果策略$`\pi`$是随机的，那获取的奖励就是随机变量，根据大数定律，随机变量会趋于期望。所以用期望评估策略要比随机变量的累加值（回报）更准些。  
个人理解，强化学习就是把问题抽象成一个随机过程，研究如果捕捉到随机过程中的规律，来获取更好的结果。

#### 2.4 为什么贝尔曼方程即时奖励期望和未来奖励期望两部分得到的$`p(?|s,a)`$一个是按$`s'`$展开一个是按r展开？

有这个疑问是因为这里的展开方式和《深度学习入门4：强化学习》的展开方式不同。那本书里把奖励看做一个以$`s、a、s'`$为输入的函数：
```math
r(s,a,s')
```
并没有概率分布。但是本书引入了针对$r$的的概率分布：
```math
p(r|s,a)
```
所以仔细看公式，两本书的推导是不同的。  

仔细看贝尔曼方程的两部分。  
即时奖励部分，由于只涉及当前状态、动作，和奖励的分布，所以公式是通过奖励分布展开。  
而未来奖励部分，由于涉及了未来状态的转移，所以必须引入状态转移概率分布：$`p(s'|s,a)`$.  
最终推导完，两部分各自出现了$`p(r|s,a)`$和$`p(s'|s,a)`$。通过联合概率与边缘概率的关系公式：  
```math
p(s'|s,a) = \sum_{r \in \mathcal{R}} p(s', r | s, a)
```
```math
p(r|s,a) = \sum_{s' \in \mathcal{S}} p(s', r | s, a)
```
统一了两个概率分布到公共部分。  

这个问题描述的不清楚，有点模糊，可能还缺少一些推导。  

#### 公式展开疑问
```math
\mathbb{E}[G_{t+1}|S_{t}=s]=\sum_{s^{\prime}\in\mathcal{S}}\mathbb{E}[G_{t+1}|S_{t}=s,S_{t+1}=s^{\prime}]p(s^{\prime}|s)
```
这里推导依赖的是条件期望的全期望定律。有点没绕过来，mark一下。  

#### 推导下贝尔曼方程
talk is cheap，不推导一下后续变换的时候总是不能快速反应过来。  

```math
v_\pi(s) = \mathbb{E}(G_t|S_t=s)

```
```math
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}....\\
G_{t+1} = R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4}...
```
so
```math
G_t = R_{t+1}+\gamma G_{t+1}
```
so
```math
v_\pi(s) = \mathbb{E}(R_{t+1}+\gamma G_{t+1}|S_t=s)
```
由于期望是线性的，于是分成两部分：
```math
v_{\pi}(s) = \mathbb{E}(R_{t+1}|S_t=s) + \gamma \mathbb{E}(G_{t+1}|S_t=s)
```
前半部分是即时奖励，后半部分是未来奖励。先看即使奖励：
```math
\mathbb{E}(R_{t+1}|S_t=s)\\
=\sum_{a\in\mathcal{A}}\pi(a|s)\mathbb{E}(r|S_t=s,A_t=a)\\
=\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{r\in\mathcal{R}}p(r|S_t=s,A_t=a)r
```

*推导过程中发现一个错误写法，把书中的$`S_t=s`$写成了$`s=S_t`$，混淆了符号。$`S_t`$是t时刻的随机变量，而s是随机变量的取值。反过来是错误的写法*