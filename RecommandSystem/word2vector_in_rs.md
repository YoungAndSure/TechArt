# word2vector in RS

在推荐系统做了几年工程，总觉得知其然而不知其所以然，常常苦恼。近年陆续涉猎了深度学习、自然语言处理、强化学习的基本原理，再回过头来看推荐系统，多了一些理解。但是看推荐系统的最新论文，感觉理解还是不够深入，就是：你知道它用这个数据乘了那个数据，然后做了这个变换，但是为什么可以这样呢？为什么这样就能行呢？说不清楚。只知其表，不知其理，终究还是经不住推敲。  
  
追根溯源，读经典论文DNN，发觉，模型结构和CBOW有共通之处，文中也说借鉴了word2vector。再加上推荐系统的特点：巨大的嵌入表，足以说明，word2vector对推荐系统模型的影响之深。所以，还是要从word2vector理解起。  
  
而word2vector的前身是矩阵分解。  

## 自然语言处理的演化思路
### 矩阵分解
重新略读鱼书之三中一二章，总结自然语言处理的矩阵分解法思路：
- 提出分布假设
- 用共现矩阵在计算机上表示分布假设
- 通过余弦相似度评估词间相似度

以上，实际就完成了一次粗略的矩阵分解建模。它实际上回答了这么几个问题：
- 如何表示一个词的含义？分布假设：一个词的含义由上下文决定
- 如何在计算机中表示分布假设？共现矩阵
- 如何在计算机中表示词？共现矩阵中的词对应的列
- 如何衡量词之间的相似度？余弦相似度计算

以上每个问题的解法并不唯一，比如衡量词向量的相似度，就有很多种方法。但这些问题给出了一个思考框架，后续的改进都是在这个思考框架之下的改进。比如：  
- 高频词噪声——共现矩阵改良为PPMI矩阵
- 矩阵稀疏，向量维度高——奇异值分解

一个是降低词向量的噪声，一个是降低词向量维度，以节省内存和计算量。  
>插一嘴：由于大厂的分工很细，比如经常分为“工程”和“算法”，导致很容易形成“算法不用考虑工程”或“工程不用理解算法”的风气。实际上，算法和工程从来没有、也不应该分家。比如这里的向量降维，就是利用算法解决大规模工程落地的问题。大厂这么做的本意，是想把算法从繁琐的实现细节中剥离出来，以更好的发挥算法上的创造优势，但实际上，脱离了实现细节的算法，就是空中楼阁，很可能无法落地。把职位、甚至部门分开，更增加了算法和实现的隔阂，就好像脑子指挥不了胳膊。说到底，只应该存在一种职位，就是“应用算法工程师”。任何希望在这个方向上有所进步的人，都应该算法工程双修。

需要注意的是，这里的建模，纯靠数学推导，这里对于建模后两个词向量的相似性，没有一个明确的指标衡量，全靠人感觉。

### 降维
>SVD可以用来降维，但不是用来降维的，鱼书在这一点上有误导
 

鱼书中对SVD的介绍比较简单，这里稍微详细的展开说下。但只介绍流程，不介绍数学原理。  
SVD是说，一个矩阵可以分解为三个矩阵的乘积：
```math
X = USV^T
```
假设词表长度为m，则共现矩阵就是一个$`(m, m)`$大小的矩阵$`X_{(m,m)}`$。可以分解为：
```math
X_{(m,m)} = U_{(m,m)}S_{(m,m)}V_{(m,m)}^T
```
注意，分解出的$`U_{(m,m)},S_{(m,m)},V_{(m,m)}`$维度均为$`(m,m)`$，其中$`S_{(m,m)}`$是一个只有对角有值的矩阵，称之为奇异值矩阵。  
降维操作就是取$`U_{(m,m)}`$矩阵的前$`k`$列$`U_{(m,k)}`$代表$`X_{(m,m)}`$矩阵。  
于是，某词one_hot编码为$`i`$，则SVD分解前的词向量为$`X_{(m,m)}`$第$`i`$
行，分解后为$`U_{(m,k)}`$第$`i`$行，维度从$`m`$降为$`k, (k<m)`$。

 